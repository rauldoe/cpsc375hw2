1:10
seq(1,10)
seq(1,10,2)
a <- seq(1,10,2)
str(a)
b <- (10,20,30)
b <- c(10,20,30)
?c
10:-2:1
list(name="test", age=45)
c <- list(name="test", age=45)
d = list(name="test", age=45)
doe = list(name="test", age=23)
doe[0]
doe[1]
doe[2]
doe[[2]]
str(doe[2])
str(doe[[2]])
doe[2:3]
View
?View
View(iris)
iris["Species"]
levels(iris["Species"])
levels(iris$Species)
levels(Species)
levels(iris["Species"])
levels(iris$Species)
iris[1,]
iris[,]
library(ggplot2)
library(ggplot)
package(ggplot2)
install.packages(ggplot2)
install.packages("ggplot2"")
install.packages("ggplot2")
library(ggplot2) # load the ggplot library
s
library(ggplot2) # load the ggplot library
d
library(ggplot2)
t <- matrix(1, 2, 3, 4)
colnames(t) <- c(x, y)
ccolnames(t) <- c("x", "y")
t <- matrix(1, 2, 3, 4, ncol=2, byrow=TRUE)
t <- matrix(c(1, 2, 3, 4), ncol=2, byrow=TRUE)
colnames(t) <- c("x", "y")
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y)) # can store plots in a variable
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y)) # can store plots in a variable
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y))
t <- data.frame("x" = c(1, 2), "y" = c(3,4))
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y))
myplot
x <-c(	45,	36,	37,	43,	38,	49,	39,	43,	44,	38,	42,	40)
x
y <-c(	43,	35,	34,	41,	44,	44,	42,	46,	39,	39,	47,	39)
t <- data.frame("x" = x, "y" = y)
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y)) # can store plots in a variable
myplot
q(0)
q()
q
library(ggplot2) # load the ggplot library
x <-c(	45,	36,	37,	43,	38,	49,	39,	43,	44,	38,	42,	40)
y <-c(	43,	35,	34,	41,	44,	44,	42,	46,	39,	39,	47,	39)
x
y
t <- data.frame("x" = x, "y" = y)
fun.1 <- function(x) 1688.840 + 0.741 * x
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y)) + stat_function(fun = fun.1)
myplot
fun.1 <- function(x) 15.426 + 0.599 * x
myplot
myplot <- ggplot(data=t) + geom_point(mapping=aes(x=x,y=y)) + stat_function(fun = fun.1)
myplot
q(0)
q()
q
ii.	survey <- read.csv("survey2019.csv")
survey <- read.csv("survey2019.csv")
setwd("C:/Users/kd/Google Drive/School/CSUF_Class/CPSC_375_data_science/hw1")
setwd("C:/Users/kd/Google Drive/School/CSUF_Class/CPSC_375_data_science/hw1")
i.	setwd("C:/Users/kd/Google Drive/School/CSUF_Class/CPSC_375_data_science/hw1")
setwd("C:/Users/kd/Google Drive/School/CSUF_Class/CPSC_375_data_science/hw1")
q
setwd("/Users/mikedo/Google Drive/School/CSUF_Class/CPSC_375_data_science/hw1")
library(ggplot2)
#install.packages("tidyverse")
library(tidyverse)
setwd("/Users/mikedo/Google Drive/School/CSUF_Class/CPSC_375_data_science/hw1")
survey <- read.csv("survey2019.csv")
seq <- 1:10
mathList = survey %>% group_by(Math) %>% summarise(Count=n()) %>% mutate(Item=Math, Discipline="Math") %>% select(Item, Count, Discipline) %>% arrange(Item)
#mathList <- mathList %>% complete(Item = seq, fill = list(Count = 0, Discipline="Math"))
csList = survey %>% group_by(CS) %>% summarise(Count=n()) %>% mutate(Item=CS, Discipline="CS") %>% select(Item, Count, Discipline) %>% arrange(Item)
#csList <- csList %>% complete(Item = seq, fill = list(Count = 0, Discipline="CS"))
#totalList <- merge(mathList, csList, by="Item")
totalList <- bind_rows(mathList, csList)
#scatter plot
p <- ggplot(data = totalList)
graph =
p +
geom_point(mapping=aes(x=totalList$Item, y=totalList$Count, colour=totalList$Discipline)) +
ggtitle("Q1 Survey of Students With Their Levels of Math and CS") +
theme(plot.title = element_text(hjust = 0.5)) +
xlab("Skill Levels") +
ylab("# of Students") +
scale_x_discrete(breaks=totalList$Item, labels=as.character(totalList$Item), limits=totalList$Item) +
theme(legend.position = c(0.95, 0.95), legend.justification = c("right", "top")) +
labs(fill = "Dose (mg)")
pdf("question2d_graph.pdf")
print(graph)
dev.off()
str(iris)
sample(1:10, 1) # function to return a random number
sample(1:100, 1)
sample(1:100, 5)
trainindex = sample(1:150, 100) # don't pick 1:100
trainfeatures = iris[trainindex,1:4]
View(trainfeatures)
View(iris)
trainlabels = iris[trainindex, "Species"]
str(trainlabels)
trainlabels[1]
trainlabels[0]
trainlabels[-1]
trainlabels[0]
x <- [1, 2, 3]
x <- c(1, 2, 3)
x[0]
x[1]
trainlabels[1]
trainlabels[0]
trainlabels[1]
trainlabels[2]
trainlabels
trainlabels[3]
trainlabels[4]
predictedlabels <- knn(trainfeatures=trainfeatures, cl=trainlabels, test=testfeatures, k=1)
library(class)
predictedlabels <- knn(trainfeatures=trainfeatures, cl=trainlabels, test=testfeatures, k=1)
predictedlabels <- knn(trainfeatures=trainfeatures, cl=trainlabels, test=testfeatures, k=1)
trainfeatures = iris[trainindex,1:4]
predictedlabels <- knn(trainfeatures=trainfeatures, cl=trainlabels, test=testfeatures, k=1)
predictedlabels <- knn(train=trainfeatures, cl=trainlabels, test=testfeatures, k=1)
trainlabels = iris[trainindex, "Species"]
testindex = setdiff(1:150, trainindex) #101:150
testfeatures = iris[testindex, 1:4]
predictedlabels <- knn(train=trainfeatures, cl=trainlabels, test=testfeatures)
predictedlabels
predictedlabels <- knn(train=trainfeatures, cl=trainlabels, test=testfeatures, k=1)
# install.packages("class")
library(tidyverse)
library(ggplot2)
library(dplyr)
library(class)
setwd("c:/temp/cpsc375hw2")
# a. Load and pre-process the data. Show code to:
#   i. Load the data file on Titanium.
data <- read_csv("data_banknote_authentication.csv")
str(data)
#   ii. How many rows and columns are there?
nrow(data)
ncol(data)
# 1, 3, 5, ... for training
# odd
traindata <- data %>% dplyr::filter(row_number() %% 2 == 1)
# remaining rows for test (i.e, test using rows 2, 4, 6, .)
# even
testdata <- data %>% dplyr::filter(row_number() %% 2 == 0)
# c. Train and test a k-nearest neighbor classifier with the above datasets. Consider only
#  variance and skewness columns. Set k=1. What is the error rate (number of
#  misclassifications)? (code)
trainfeatures <- traindata[1:4]
trainlabels <- factor(traindata$forged)
testfeatures <- testdata[1:4]
testlabels <- factor(testdata$forged)
predictedlabels <- knn(train = trainfeatures, cl = trainlabels, test=testfeatures, k = 1)
actualVsPredicted <- table(testlabels, predictedlabels)
actualVsPredicted
errorRate <- sum(actualVsPredicted) - sum(diag(actualVsPredicted))
# install.packages("class")
library(tidyverse)
library(ggplot2)
library(dplyr)
library(class)
setwd("c:/temp/cpsc375hw2")
# a. Load and pre-process the data. Show code to:
#   i. Load the data file on Titanium.
data <- read_csv("data_banknote_authentication.csv")
str(data)
#   ii. How many rows and columns are there?
nrow(data)
ncol(data)
# b. Split the dataset into train and test datasets with the rows 1, 3, 5, ... for training, and the
#  remaining rows for test (i.e, test using rows 2, 4, 6, .). Do NOT randomly sample the
#  data (though resampling is usually done, this hw problem does not use this step for ease
#  of grading). (code)
# 1, 3, 5, ... for training
# odd
traindata <- data %>% dplyr::filter(row_number() %% 2 == 1)
# remaining rows for test (i.e, test using rows 2, 4, 6, .)
# even
testdata <- data %>% dplyr::filter(row_number() %% 2 == 0)
colsToConsider <- 1:3
trainfeatures <- traindata[colsToConsider]
trainlabels <- factor(traindata$forged)
testfeatures <- testdata[colsToConsider]
testlabels <- factor(testdata$forged)
trainfeatures$variance.scaled <- scale(trainfeatures$variance)
trainfeatures$skewness.scaled <- scale(trainfeatures$skewness)
trainfeatures$curtosis.scaled <- scale(trainfeatures$curtosis)
testfeatures$variance.scaled <- scale(testfeatures$variance)
testfeatures$skewness.scaled <- scale(testfeatures$skewness)
testfeatures$curtosis.scaled <- scale(testfeatures$curtosis)
# adjust the cols to use the scaled columns
colsToConsider <- 4:6
trainfeatures <- trainfeatures[colsToConsider]
testfeatures <- testfeatures[colsToConsider]
# i. Train and test a k-nearest neighbor classifier with the normalized dataset. Consider only
# variance, skewness, and curtosis columns. Set k=1. What is the error rate?
k <- 1
predictedlabels <- knn(train = trainfeatures, cl = trainlabels, test=testfeatures, k = k)
actualVsPredicted <- table(testlabels, predictedlabels)
actualVsPredicted
errorRate <- sum(actualVsPredicted) - sum(diag(actualVsPredicted))
library(tidyverse)
library(ggplot2)
setwd("c:/temp/cpsc375hw2")
# a. Load the autompg.csv file on Titanium and convert cylinders variable to a factor.
# (code, output of str())
data <- read_csv("autompg.csv")
cylinderData <- factor(data$cylinders)
str(cylinderData)
# b. Which is the dependent variable? Which are the independent variables?
# c. Plot mpg vs. displacement (code, plot)
g <- ggplot(data = data)
p <- g +
geom_point(mapping = aes(x = mpg, y = displacement))
pdf("hw2q1c_graph.pdf")
print(p)
dev.off()
p
m <- lm(mpg~displacement, data = data)
View(m)
m$coefficients
summary(m)
m$coefficients
library(tidyverse)
View(mpg)
mpg[1:50,]  # subsetting using R base
mpg[mpg$cty > 25,]  # subsetting using R base
filter(mpg, cty > 25) # subsetting using tidyverse
filter(mpg, cty > 25, displ > 1.5) # two conditions
filter(mpg, cty > 20, displ > 1.5, manufacturer == "audi") # three conditions
filter(mpg, cty > 25 | displ > 2) # OR of two conditions
select(mpg, manufacturer, cty) # selecting columns
select(filter(mpg, cty > 25), manufacturer) # first filter, then select
filter(select(mpg,manufacturer), cty > 25) # does not work. (Why?)
mpg %>% filter(cty > 25) %>% select(manufacturer) # filter, then select using data pipelines
mpg  %>% select(manufacturer) %>% filter(cty > 25) # does not work. (Why?)
mpg  %>% select(starts_with("m")) # select helper function
mpg  %>% select(manufacturer:trans) # select helper function
mpg  %>% select(-trans) # select helper function
mpg  %>% arrange(cty) # sorting
mpg  %>% arrange(cty, hwy)
mpg  %>% arrange(desc(cty))
# more efficient to first filter, then select
mpg %>% select(manufacturer,model,cty) %>% filter(cty > 20)
mpg %>% select(manufacturer,model,cty) %>% filter(cty > 20, hwy > 20)
mpg %>% select(manufacturer,model,cty,hwy) %>% filter(cty > 20, hwy > 20)
mpg %>% select(manufacturer,model,cty,hwy) %>% filter(cty > 20, hwy > 20) | select(manufacturer, model, cty)
mpg %>% select(manufacturer,model,cty,hwy) %>% filter(cty > 20, hwy > 20) %>% select(manufacturer, model, cty)
mpg %>% filter(cty > 20, hwy > 20) %>% select(manufacturer,model,cty)
mpg %>% filter(cty > 27, hwy > 15) %>% select(manufacturer,model,cty) %>% arrange(cty)
mpg %>% filter(cty > 27, hwy > 15) %>% select(manufacturer,model,cty, hwy) %>% arrange(hwy)
mpg %>% filter(cty > 27, hwy > 15)  %>% arrange(hwy) %>% select(manufacturer,model,cty)
# adding a new column
mpg %>% mutate(totalmpg=cty+hwy)
View(mpg %>% mutate(totalmpg=cty+hwy))
# use new column for sorting
mpg %>% filter(cty > 27, hwy > 15)  %>% mutate(totmpg=cty+hwy) %>% arrange(totmpg) %>% select(manufacturer,model,cty,hwy)
# code from 9/17/2019:
mpg %>% filter(manufacturer=="honda", cty>25)
mpg %>% select(model,class) %>% filter(manufacturer=="honda", cty>25)
mpg %>% filter(manufacturer=="honda", cty>25)%>% select(model,class)
mpg %>% filter(manufacturer=="honda", cty>25)%>%arrange(cty)
mpg %>% filter(manufacturer=="honda", cty>25)%>%arrange(cty) %>% select(model,class,cty)
mpg %>% filter(manufacturer=="honda", cty>25)%>%arrange(desc(cty)) %>% select(model,class,cty)
mpg %>% filter(manufacturer=="honda", cty>25) %>% mutate(newvar=(cty+hwy)/2) %>%arrange(desc(cty)) # adding new column with average mileage: (cty+hwy)/2
mpg %>% filter(manufacturer=="honda", cty>25) %>% mutate(newvar=(cty+hwy)/2) %>%arrange(desc(newvar)) %>% select(model,class,cty,hwy,newvar)
mpg %>% summarise(mean(cty))
mpg %>% summarise(mean(cty), mean(hwy))
mpg %>% filter(manufacturer=="honda") %>% summarise(mean(cty), mean(hwy))
mpg %>% filter(manufacturer=="honda") %>% summarise(mean(cty))
mpg %>% filter(manufacturer=="honda", manufacturer=="toyota") %>% summarise(mean(cty)) # manufacturer = honda AND toyota
mpg %>% filter(manufacturer=="honda"| manufacturer=="toyota") %>% summarise(mean(cty)) # manufacturer = honda OR toyota
mpg %>% filter(manufacturer %in% c("honda", "toyota")) %>% summarise(mean(cty))  # manufacturer is one of the list (honda,toyota)
mpg %>% group_by(manufacturer) %>% summarise(mean(cty))
mpg %>% group_by(manufacturer,class) %>% summarise(mean(cty))
mpg %>% group_by(manufacturer,class) %>% summarise(n()) # Use function n() to count rows
library(tidyverse)
# visualize the variables to model
ggplot(data=iris) + geom_point(mapping=aes(x=Sepal.Width, y=Sepal.Length))
# linear model
lm(data=iris, Sepal.Length~Sepal.Width)
# save model to a variable
mod <- lm(data=iris, Sepal.Length~Sepal.Width)
# print the model parameters and evaluation metric
summary(mod)
# model parameters
mod$coefficients
mod$coefficients[1] # slope
mod$coefficients[2] # intercept
# model parameters
mod$coefficients
mod$coefficients[1] # slope
mod$coefficients[2] # intercept
ggplot(data=iris) + geom_point(mapping=aes(x=Sepal.Width, y=Sepal.Length))
# model parameters
mod$coefficients
mod$coefficients[1] # slope
mod$coefficients[2] # intercept
# plot best fit line over the scatterplot
ggplot(data=iris) + geom_point(mapping=aes(x=Sepal.Width, y=Sepal.Length)) + geom_abline(slope=mod$coefficients[2], intercept = mod$coefficients[1])
# model another pair of variables
mod <- lm(data=iris, Petal.Length~Petal.Width)
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length)) + geom_abline(slope=mod$coefficients[2], intercept = mod$coefficients[1])
summary(mod)
library(modelr)
irisCopy <- iris %>% add_residuals(mod)
irisCopy <- iris %>% add_residuals(mod)
View(irisCopy)
# check if residuals are normally distributed: plot its histogram
ggplot(data=irisCopy) + geom_histogram(mapping = aes(x=resid))
ggplot(data=irisCopy) + geom_histogram(mapping = aes(x=irisCopy$resid))
ggplot(data=irisCopy) + geom_histogram(mapping = aes(x=irisCopy$resid)))
ggplot(data=irisCopy) + geom_histogram(mapping = aes(x=irisCopy$resid))
ggplot(data=irisCopy) + geom_histogram(mapping = aes(x=resid))
irisCopy <- iris %>% add_residuals(mod)
ggplot(data=irisCopy) + geom_histogram(mapping = aes(x=resid))
library(tidyverse)
# visualize the variables to model
ggplot(data=iris) + geom_point(mapping=aes(x=Sepal.Width, y=Sepal.Length))
# linear model
lm(data=iris, Sepal.Length~Sepal.Width)
# save model to a variable
mod <- lm(data=iris, Sepal.Length~Sepal.Width)
# print the model parameters and evaluation metric
summary(mod)
# model parameters
mod$coefficients
mod$coefficients[1] # slope
mod$coefficients[2] # intercept
# plot best fit line over the scatterplot
ggplot(data=iris) + geom_point(mapping=aes(x=Sepal.Width, y=Sepal.Length)) + geom_abline(slope=mod$coefficients[2], intercept = mod$coefficients[1])
# model another pair of variables
mod <- lm(data=iris, Petal.Length~Petal.Width)
ggplot(data=iris) + geom_point(mapping=aes(x=Petal.Width, y=Petal.Length)) + geom_abline(slope=mod$coefficients[2], intercept = mod$coefficients[1])
summary(mod)
# add residuals to the dataset
library(modelr)
irisCopy <- iris %>% add_residuals(mod)
View(irisCopy)
ggplot(data=irisCopy) + geom_histogram(mapping = aes(x=resid))
