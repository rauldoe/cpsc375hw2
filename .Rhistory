install.packages("tidyverse")
install.packages('ggplot2')
# a. Load and pre-process the data. Show code to:
#   i. Load the data file on Titanium.
data <- read_csv("data_banknote_authentication.csv")
library(tidyverse)
library(ggplot2)
setwd("c:/temp/cpsc375hw2")
# a. Load and pre-process the data. Show code to:
#   i. Load the data file on Titanium.
data <- read_csv("data_banknote_authentication.csv")
#   ii. How many rows and columns are there?
nrow(data)
ncol(data)
# b. Split the dataset into train and test datasets with the rows 1, 3, 5, ... for training, and the
#  remaining rows for test (i.e, test using rows 2, 4, 6, â€¦). Do NOT randomly sample the
#  data (though resampling is usually done, this hw problem does not use this step for ease
#  of grading). (code)
# c. Train and test a k-nearest neighbor classifier with the above datasets. Consider only
#  variance and skewness columns. Set k=1. What is the error rate (number of
#  misclassifications)? (code)
View(iris)
lm(iris$Sepal.Length~iris$Petal.Width)
m <- lm(iris$Sepal.Length~iris$Petal.Width)
m
summary(m)
sumr <- summary(m)
sumr
m$coefficients
c <- m$coefficients
c
c
residuals <- y - (x*c[2] + c[1])
residuals(m)
fitted.values(m)
p <- ggplot(data=iris)
g <- geom_abline(slope=c[2], intercept = c[1])
g
g
View(g)
add_residuals(m)
library(modelr)
mygrid <- mpg %>% data_grid(displ)
mygrid
View(mygrid)
View(mpg)
mygrid <- mpg %>% data_grid(mpg$displ)
View(mygrid)
mygrid <- mpg %>% data_grid(displ)
View(mygrid)
ggplot(data=iris) + geom_point(mapping=aes(x=Sepal.Width, y=Sepal.Length)) + geom_abline(slope=mod$coefficients[2], intercept = mod$coefficients[1])
mod <- lm(data=iris, Sepal.Length~Sepal.Width)
ggplot(data=iris) + geom_point(mapping=aes(x=Sepal.Width, y=Sepal.Length)) + geom_abline(slope=mod$coefficients[2], intercept = mod$coefficients[1])
clas
class
library(class)
install.packages("class")
